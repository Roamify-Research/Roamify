\documentclass[conference]{IEEEtran}
\usepackage{natbib}
\usepackage{url}
\usepackage{hyperref}


\ifCLASSINFOpdf

\else

\fi

\hyphenation{op-tical net-works semi-conduc-tor}


\begin{document}

\title{\textbf{Roamify: Roaming Redefined}\\ \textit{Changing the Way World Travels}}


\author{\IEEEauthorblockN{Vikranth Udandarao}
\IEEEauthorblockA{Computer Science Engineering Dept\\
IIIT Delhi\\
Email: vikranth22570@iiitd.ac.in}
\and
\IEEEauthorblockN{Harsh Mistry}
\IEEEauthorblockA{Computer Science Engineering Dept\\
IIIT Delhi\\
Email: harsh22200@iiitd.ac.in}
\and
\IEEEauthorblockN{Muthuraj Vairamuthu}
\IEEEauthorblockA{Computer Science Engineering Dept\\
IIIT Delhi\\
Email: muthuraj22307@iiitd.ac.in}
\and
\IEEEauthorblockN{Noel Tiju}
\IEEEauthorblockA{Computer Science Engineering Dept\\
IIIT Delhi\\
Email: noel22338@iiitd.ac.in}
\and
\IEEEauthorblockN{Armaan Singh}
\IEEEauthorblockA{Computer Science Engineering Dept\\
IIIT Delhi\\
Email: armaan22096@iiitd.ac.in}
\and
\IEEEauthorblockN{Dhruv Kumar}
\IEEEauthorblockA{Computer Science Engineering Dept\\
IIIT Delhi\\
Email: dhruv.kumar@iiitd.ac.in}}



% \author{
%     \IEEEauthorblockN{Vikranth Udandarao} \vspace*{3.0pt}
%     \IEEEauthorblockA{
%         \textit{Computer Science \& Engineering Dept.} \\
%         \textit{IIIT-Delhi, India} \\
%         vikranth22570@iiitd.ac.in
%     }
%     \and
%     \IEEEauthorblockN{Harsh Mistry} \vspace*{3.0pt}
%     \IEEEauthorblockA{
%         \textit{Computer Science \& Engineering Dept.} \\
%         \textit{IIIT-Delhi, India} \\
%         harsh22200@iiitd.ac.in
%     }
%     \IEEEauthorblockN{Muthuraj Vairamuthu} \vspace*{3.0pt}
%     \IEEEauthorblockA{
%         \textit{Computer Science \& Engineering Dept.} \\
%         \textit{IIIT-Delhi, India} \\
%         muthuraj22...@iiitd.ac.in
%     }
%     \and
%     \IEEEauthorblockN{Noel Tiju} \vspace*{3.0pt}
%     \IEEEauthorblockA{
%         \textit{Computer Science \& Engineering Dept.} \\
%         \textit{IIIT-Delhi, India} \\
%         noel22338@iiitd.ac.in
%     }
%     \and
%     \IEEEauthorblockN{Armaan Singh} \vspace*{3.0pt}
%     \IEEEauthorblockA{
%         \textit{Computer Science \& Engineering Dept.} \\
%         \textit{IIIT-Delhi, India} \\
%         armaan22096@iiitd.ac.in
%     }
% }



% \author{\IEEEauthorblockN{Vikranth Udandarao\IEEEauthorrefmark{1},
% Harsh Mistry\IEEEauthorrefmark{1},
% Muthuraj Vairamuthu\IEEEauthorrefmark{1}, 
% Noel Tiju\IEEEauthorrefmark{1},
% Armaan Singh\IEEEauthorrefmark{1}}
% \IEEEauthorblockA{\IEEEauthorrefmark{1}Computer Science Engineering Dept\\
% IIIT Delhi\\
% }}




\maketitle

\begin{abstract}

    Travelling without planning is a challenge for travellers. This paper delves into the heart of a common dilemma: the desire to explore without the hassle of planning. Roamify, an AI-generated itinerary service, empowers travellers with curated suggestions tailored to their destination preferences. It is aimed at empowering travellers with personalized experiences along the way. Taking inspiration from personal experiences grappling with itinerary complexities, it emerges as an innovative solution, leveraging artificial intelligence (AI) generated itineraries to empower travellers with customized recommendations tailored to the destination's preferences. This introduction lays the foundation for an exploration into how Roamify redefines roaming, making travel not just about reaching a place but also about unlocking memorable experiences along the way.

\end{abstract}

\IEEEpeerreviewmaketitle

\section{Introduction}

    \textbf{Roamify} - Roaming Redefined is on a transformative mission to revolutionize travel planning for today's generation. Aimed at youngsters, teenagers, college students, and families seeking seamless travel experiences, Roamify addresses a common dilemma: the desire to explore without the hassle of planning. Drawing inspiration from personal experiences with itinerary complexities, Roamify emerges as an innovative solution, leveraging AI-generated itineraries to provide curated suggestions tailored to travelers' destination preferences. This introduction sets the stage for exploring how Roamify redefines roaming, making travel not just about reaching a destination but also about unlocking memorable experiences along the way.

    In the past, travel was a luxury limited to the rich and elite. However, with advancements in transportation, travel has become more accessible to everyone. People now travel for various reasons: work, leisure, relaxation, reunions with friends, or simply for the joy of exploring new places. This increase in travel purposes has led to a new challenge: deciding what to explore upon arrival.

    Imagine a family of four planning a vacation during the kids' summer holidays. In the past, their only option was to visit an offline travel agency to book a package that included all the itineraries and attractions. With technological advancements, numerous online platforms have emerged, allowing people to customize and choose their own itineraries.

    Despite these advancements, a significant problem persists: many online travel websites struggle to attract users, even after being in existence for several years. Additionally, while tools like ChatGPT offer some assistance, they have limitations, such as outdated training data.

    Roamify aims to address these issues by integrating seamlessly with users' web browsers. Recognizing that travelers often have multiple tabs open while planning their trips, Roamify leverages this browsing behavior to provide intelligent recommendations based on the websites and tabs users have open. This innovative approach ensures that travel planning becomes more efficient and personalized, making Roamify an essential tool for modern travelers.

    The development and fine-tuning of the models used in Roamify were meticulously documented and are available on our GitHub repository: \href{https://github.com/RoamifyRedefined/Machine-Learning}{\textbf{RoamifyRedefined/Machine-Learning}}.

\section{Literature Review}

    In recent years, advancements in natural language processing (NLP) have significantly enhanced the ability of machine learning models to understand and generate human language. These advancements have been particularly beneficial for applications requiring detailed textual analysis and generation, such as travel itinerary planning. This paper presents a comparative study of five state-of-the-art NLP models—BERT, T5, DistilBERT, Llama, and RoBERTa—fine-tuned to generate personalized travel itineraries. By leveraging these models, we aim to redefine the travel planning experience through Roamify, an AI-powered itinerary generation service.

    \subsection{BERT (Bidirectional Encoder Representations from Transformers)}
        BERT, developed by Google, is a transformer-based model designed to understand the context of a word in search queries. Its bidirectional approach allows it to consider both the left and right context of a word, making it highly effective for tasks such as question answering and language understanding. In this study, we fine-tuned BERT for the task of generating travel itineraries, leveraging its deep understanding of context to provide precise and relevant recommendations.

    \subsection{T5 (Text-To-Text Transfer Transformer)}
        T5, also developed by Google, is a versatile model that treats all NLP tasks as a text-to-text problem. This unified approach allows T5 to be applied to a wide range of tasks by simply converting them into text generation tasks. For Roamify, we fine-tuned T5 to generate detailed travel itineraries based on user preferences and historical travel data, capitalizing on its ability to generate coherent and contextually accurate text.

    \subsection{DistilBERT}
        DistilBERT is a smaller, faster, and more efficient version of BERT, retaining 97\% of its language understanding capabilities while being 60\% faster and 40\% smaller. This model was fine-tuned to perform similar tasks as BERT but with reduced computational requirements, making it a viable option for real-time applications in travel itinerary generation.

    \subsection{Llama (LLaMA: Large Language Model with Attention)}
        Llama is a highly sophisticated language model known for its ability to handle large-scale language understanding and generation tasks. Utilizing the Unsloth framework for fine-tuning, we applied Llama to generate detailed and personalized travel itineraries. Its extensive pre-training and fine-tuning capabilities make it particularly well-suited for handling complex and nuanced user queries in the travel domain.

    \subsection{RoBERTa (Robustly Optimized BERT Pretraining Approach)}
        RoBERTa is an optimized version of BERT, designed to improve performance on various NLP tasks by modifying key hyperparameters, removing the next sentence prediction objective, and training with much larger mini-batches and learning rates. For Roamify, we used RoBERTa to enhance the contextual understanding of travel-related queries, leveraging its advanced question answering capabilities to provide accurate and relevant responses.

    By evaluating these models, we aim to determine the most effective approach for generating personalized travel itineraries. The following sections detail the methodology, model configurations, training processes, and evaluation metrics used in this comparative study.

\section{HTML Content Extraction and Itinerary Generation}

    In this study, we implemented a Chrome extension to enhance our travel itinerary generation process. The extension identifies locations from open browser tabs using airport codes, scrapes relevant web content for these locations, and processes this data for itinerary generation.

    \subsection{HTML Content Extraction and Location Identification}

        \subsubsection{Chrome Extension Setup}
        \begin{itemize}
            \item We developed a Chrome extension named "Roamify," which runs a service worker script and displays a sidebar.
            \item The extension requests permissions to access various Chrome features, including tabs, scripting, and activeTab, allowing it to interact with and manipulate the content of open tabs.
        \end{itemize}
        
        \subsubsection{Identifying Locations}
        \begin{itemize}
            \item The extension uses JavaScript to extract the HTML content of each tab and identifies locations based on airport codes found in the URL.
            \item This is done by matching URL patterns against a predefined list of airport codes and their corresponding cities. For example, if a URL contains “BOM” and “DEL” the script will recognise these as Mumbai and Delhi, respectively.
        \end{itemize}

        \subsubsection{Content Display and Processing}
        \begin{itemize}
            \item Once the locations are identified, the extension constructs links to travel-related blogs and fetches the HTML content of these pages.
            \item The content is cleaned by removing scripts and styles, and key elements such as titles, descriptions, main content, and attractions are extracted.
            \item This extracted information is then displayed in a user-friendly format in the extension’s sidebar.
        \end{itemize}

    \subsection{Backend Integration for Itinerary Processing}

        \subsubsection{Backend API Interaction}
        \begin{itemize}
            \item The extension integrates with a backend API hosted locally. The API processes the scraped HTML content and returns a structured itinerary.
            \item The interaction with the API is handled using the Fetch API with POST requests, sending the cleaned and structured data to the backend.
        \end{itemize}

        \subsubsection{Processing and Displaying Results}
        \begin{itemize}
            \item The backend API processes the data, extracting meaningful information and generating travel itineraries based on the content.
            \item The response from the API is then displayed in the extension’s sidebar, providing users with a detailed and organised travel plan.
        \end{itemize}

    \subsection{Summary}
        Combining browser automation, web scraping, and backend processing, the Roamify extension streamlines the process of generating travel itineraries. The extension identifies locations from open browser tabs using airport codes, scrapes relevant content from travel blogs, and processes this information through a backend API to provide users with comprehensive travel plans. This approach enhances the user experience by leveraging AI and NLP technologies to deliver personalised travel recommendations, ensuring users can access detailed and relevant travel information directly within their browser.

\section{NLP Processing}

    This algorithm was used to clear junk from the scraped data. It involved four stages:

    \subsection{Using spaCy}
    We utilized the \texttt{en\_core\_web\_lg} model provided by the \texttt{spaCy} library to preprocess the data obtained through web scraping. The \texttt{en\_core\_web\_lg} model is a comprehensive English language model that incorporates pre-trained word vectors and has the ability to execute a range of natural language processing tasks, including tokenization, part-of-speech tagging, named entity recognition, and dependency parsing. By performing this pre-processing phase, we were able to efficiently cleanse and organize the data.

    \subsection{Extracting Sentences}
    We obtained sentences from the text that was processed using \texttt{spaCy}. During this step, the continuous text was divided into separate sentences, which facilitated the identification and analysis of distinct pieces of information.

    \subsection{Recognizing Patterns}
    Next, we segmented the sentences into individual words using the \texttt{word\_tokenize} feature. Throughout this procedure, our objective was to identify a particular pattern: a consecutive series of integers commencing with 1. This pattern was clearly identifiable in the data obtained through web scraping from websites that provide recommendations for attractions.

    \subsection{Data Processing and Structuring}
    After discovering this pattern, we retrieved the text that appeared between two consecutive integers. This material usually contained elaborate descriptions or information regarding the attractions. The recovered text was refined to eliminate any remaining interference or extraneous details.

    The sanitized data was subsequently stored in a dictionary, where the keys denote the sequence numbers (1, 2, ...) and the values correspond to the purified attraction data. The structured style enabled us to effectively arrange and retrieve the pertinent information.

    \begin{verbatim}
# Example dictionary structure
{
    1: "Description of the first attraction",
    2: "Description of the second attraction",
    ...
}
    \end{verbatim}

    By adhering to these procedures, we guaranteed that the processed data was free from errors, organized, and prepared for additional analysis and model training.

\section{Methodology and Models}

    \subsection{Initial Approach}

        Initially, we utilized various models to do two distinct tasks: question answering and text-to-text generation. The models employed for each task are enumerated below:

        \subsubsection{Question Answering Models}

        For the question answering task, we employed the following models:
        \begin{itemize}
            \item BERT
            \item RoBERTa
            \item DistilBERT
        \end{itemize}

        The models were employed to address inquiries pertaining to the context presented in the dataset. The main goal was to obtain important insights and precise replies from the processed scraped data.

        \subsubsection{Text-to-Text Generation Models}

        For the text-to-text generation task, specifically summarization, we utilized the following models:
        \begin{itemize}
            \item T5
            \item Llama-3
        \end{itemize}

        The models were optimized to produce precise and succinct summaries of the provided text. The approach entailed compressing the material while preserving its significance and pertinence, which was essential for producing succinct descriptions of tourist destinations from the massive web-scraped data.
        Our objective was to improve the comprehension and utility of the gathered data by utilizing these models, hence increasing its accessibility and providing users with more interesting insights.

    \subsection{Data Collection and Preprocessing}

        The data used for fine-tuning the models was collected from various online travel platforms, focusing on popular tourist destinations. The datasets included detailed descriptions, locations, types of attractions, and user reviews. The data was pre-processed to remove any noise and irrelevant information, ensuring high-quality input for the models.

        We created two datasets for fine-tuning, each with a different purpose:

        \begin{enumerate}
            \item \textbf{Summarization Dataset}: The dataset was utilized to refine the Llama-3 and Flan-T5 models. The system was comprised of two components: context and summary. The context pertained to the data obtained by web scraping, while the summary was a concise translation of the text that was both grammatically correct and concise.
            \item \textbf{Question-Answering Dataset}: The dataset was utilized to refine Question-Answering Models, specifically BERT, DistilBERT, and RoBERTa. The document has six questions as specified in the annexure. The dataset was utilized to get significant insights from the processed scraped data.
        \end{enumerate}

        \subsubsection{Data Preparation}

            For each dataset, the following preprocessing steps were performed:
            \begin{itemize}
                \item Tokenization: Text data was tokenized using the respective model tokenizers.
                \item Cleaning: Non-ASCII characters, HTML tags, and extra whitespace were removed.
                \item Normalization: Text was converted to lowercase, and punctuation was standardized.
                \item Splitting: The data was split into training and evaluation sets with a typical ratio of 80:20.
            \end{itemize}

    \subsection{Question Answering Models}

        \subsubsection{BERT}

            BERT, developed by Google, was fine-tuned using the following configurations:
            \begin{itemize}
                \item Model: \texttt{bert-base-uncased}
                \item Tokenizer: \texttt{BertTokenizer}
                \item Training epochs: 3
                \item Learning rate: 5e-5
                \item Batch size: 8
            \end{itemize}

        \subsubsection{RoBERTa}

            RoBERTa, an optimized version of BERT, was used to enhance the contextual understanding of travel-related queries. The following configurations were used:
            \begin{itemize}
                \item Model: \texttt{deepset/roberta-base-squad2}
                \item Tokenizer: \texttt{AutoTokenizer}
                \item Training epochs: 3
                \item Learning rate: 2e-5
                \item Batch size: 8
            \end{itemize}

        \subsubsection{DistilBERT}

            DistilBERT, a distilled version of BERT, was fine-tuned with reduced computational requirements:
            \begin{itemize}
                \item Model: \texttt{distilbert-base-uncased}
                \item Tokenizer: \texttt{DistilBertTokenizer}
                \item Training epochs: 3
                \item Learning rate: 5e-5
                \item Batch size: 8
            \end{itemize}
            
    \subsection{Text2Text Generation Models}

        \subsubsection{T5}

            T5, also from Google, was treated as a text-to-text problem and fine-tuned with the following settings:
            \begin{itemize}
                \item Model: \texttt{google/flan-t5-small}
                \item Tokenizer: \texttt{AutoTokenizer}
                \item Training epochs: 10
                \item Learning rate: 2e-5
                \item Batch size: 16
            \end{itemize}

            \begin{verbatim}
Training_args = Seq2SeqTrainingArguments(
evaluation_strategy="epoch",
learning_rate=2e-5,
per_device_train_batch_size=10,
per_device_eval_batch_size=10,
weight_decay=0.01,
save_total_limit=3,
num_train_epochs=10
)
            \end{verbatim}

        \subsubsection{Llama}

            Llama, known for its large-scale language understanding capabilities, was fine-tuned using the Unsloth framework:
            \begin{itemize}
                \item Model: \texttt{unsloth/llama-3-8b-bnb-4bit}
                \item Tokenizer: \texttt{FastLanguageModel}
                \item Max steps: 30
                \item Learning rate: 2e-4
                \item Batch size: 16
                \item Use gradient checkpointing: \texttt{unsloth}
            \end{itemize}

            \begin{verbatim}
TrainingArguments(
    per_device_train_batch_size = 2,
    gradient_accumulation_steps = 4,
    warmup_steps = 5,
    max_steps = 30,
    learning_rate = 2e-4,
    fp16 = not torch.cuda.is_bf16_supported(),
    bf16 = torch.cuda.is_bf16_supported(),
    logging_steps = 1,
    optim = "adamw_8bit",
    weight_decay = 0.01,
    lr_scheduler_type = "linear",
    seed = 3407,
    output_dir = "outputs",
)
            \end{verbatim}

\section{Results and Evaluation}

    To evaluate the performance of the models, we used several metrics. These metrics assess the accuracy, overlap of n-grams, and confidence in predictions.
    \subsection{Evaluation Metrics}

        \begin{itemize}
            \item \textbf{BLEU Score:} Measures the accuracy of the generated text against reference text using n-gram precision.
            \item \textbf{ROUGE Score:} Evaluates the overlap of n-grams between the generated and reference text, particularly useful for summarization tasks.
            \item \textbf{Perplexity:} Assesses the model's confidence in its predictions, with lower values indicating better performance.
        \end{itemize}

    \subsection{Performance Comparison}

        The performance of each model was evaluated on a test set of travel itinerary data. The results are summarized in Table \ref{tab:results}.

        \begin{table}[htbp]
        \caption{Performance Comparison of Models}
        \centering
        \begin{tabular}{|l|c|c|c|}
        \hline
        \textbf{Model} & \textbf{BLEU Score} & \textbf{ROUGE Score} & \textbf{Perplexity} \\
        \hline
        T5 & 0.72 & 0.75 & 12.8 \\
        Llama & 0.78 & 0.80 & 10.4 \\
        \hline
        \end{tabular}
        \label{tab:results}
        \end{table}

    Additionally, the training and evaluation loss for DistilBERT and RoBERTa are as follows:

    \begin{itemize}
        \item \textbf{DistilBERT:}
        \begin{itemize}
            \item \textbf{Train Loss:} [1.954904317855835, 2.6523895263671875, 1.2275750637054443]
            \item \textbf{Eval Loss:} [-2.0042619705200195, -4.5487589836120605, -5.274253845214844]
        \end{itemize}

        \item \textbf{RoBERTa:}
        \begin{itemize}
            \item \textbf{Train Loss:} [1.6825495958328247, 0.762340247631073, 0.7708142995834351]
            \item \textbf{Eval Loss:} [-2.673956871032715, -4.201545715332031, -4.454132556915283]
        \end{itemize}

        \item \textbf{BERT:}
        \begin{itemize}
            \item \textbf{Train Loss:} [6.365660667419434, 6.262948036193848, 5.646426200866699],

            \item \textbf{Eval Loss:} [0.20018316805362701, 0.02616404928267002,0.05858725681900978]
        \end{itemize}
    \end{itemize}

    The negative evaluation loss scores indicate issues with evaluation, suggesting that the models were not able to derive context from the dataset properly.

    \subsection{Discussion}

        \subsubsection{Question Answering Models}

            The models' poor evaluation scores suggest that they were unable to produce precise predictions with a high level of confidence. The scores provided indicate the performance metrics of the models. Lower or negative scores indicate a lower level of prediction accuracy and a lack of reliability in the generated outputs. Despite the thorough preprocessing and meticulous fine-tuning, the models encountered difficulties in comprehending and providing effective responses to the provided environment. This result indicates that the models may have faced challenges due to the intricacy or diversity of the input data, resulting in imprecise responses and summaries. The low evaluation scores emphasize the necessity for additional improvement of the models or investigation of other methods to enhance the accuracy and confidence of predictions.

        \subsection{Text-to-Text Generation Model Inferences}

        During our evaluation of the text-to-text generation models, T5 and Llama-3, several key observations were made:

        \begin{enumerate}
            \item \textbf{Speed and Loading Time}:
            T5 had markedly superior speed compared to Llama-3 and necessitated less loading time. The high processing speed of this system is beneficial for applications that require fast generation of summaries or responses.
            
            \item \textbf{Handling of Content}:
            T5 sometimes omitted the name of the attraction in its output summary. On the other hand, Llama-3 constantly initiated its outputs by stating the name of the attraction, so guaranteeing a more distinct and organized delivery of information.
            
            \item \textbf{Grammatical Accuracy}:
            T5 shown improved ability in preserving grammatical precision in its developed conclusions. The results generated by T5 exhibited a higher level of refinement and syntactic accuracy, hence improving the readability and comprehension for end-users.
        
        \end{enumerate}

        For example, when provided with the context:
        \begin{quote}
        ‘Cubbon Park Sarangib for Pixabay Situated over a sprawling 300 acres of land the park was constructed by Richard Sankey This massive green park along with lawns deserves a special mention Offering statues of famous personalities the park is one among the popular places to visit in Bangalore with friends Location Kasturba Road Behind High Court of Karnataka Ambedkar Veedhi Sampangi Rama Nagara BangaloreTimings Open on all daysEntry Fee No entry fee Suggested Read Resorts Near Bangalore’
        \end{quote}

        The output from T5:
        \begin{quote}
        ‘Cubbon Park, a sprawling 300 acres of land, is a popular place to visit in Bangalore with friends. Offering statues of famous personalities, it is a popular place to visit with friends.’
        \end{quote}

        While Llama-3 produced:
        \begin{quote}
        ‘The park was constructed by Richard Sankey. This massive green park along with lawns deserves a special mention. Offering statues of famous personalities, the park is one among the popular places to visit in Bangalore with friends. Location: Kasturba Road, Behind High Court of Karnataka.’
        \end{quote}

    These findings emphasize the advantages and disadvantages of each model in summarizing written content, indicating factors to consider when using them in real-life situations.

    \subsection{Conclusion}

        The evaluation of these models highlights the potential of advanced NLP techniques in enhancing the travel planning experience. While each model has its strengths, Llama emerged as the best-performing model, providing highly accurate and confident travel itinerary recommendations. Future work will focus on further optimizing these models and exploring additional applications in the travel industry.

\section{Conclusion}

    This study explored the use of five state-of-the-art NLP models—BERT, T5, DistilBERT, Llama, and RoBERTa—for generating personalized travel itineraries through Roamify, an AI-powered itinerary generation service. The comparative analysis of these models provided valuable insights into their performance and suitability for the task.

    \subsection{Key Findings}

        \begin{itemize}
            \item \textbf{BERT:} Demonstrated strong performance in understanding and generating relevant travel itineraries, but faced challenges in answering questions in depth due to improperly formatted data. It had lower computational requirements but lower accuracy.
            \item \textbf{T5:} Outperformed BERT in terms of BLEU and ROUGE scores, showcasing its ability to generate coherent and contextually accurate travel itineraries. Its lower perplexity score indicates higher confidence in its predictions. It was faster than Llama.
            \item \textbf{DistilBERT:} While more efficient, DistilBERT showed slightly lower performance compared to BERT and T5. It faced issues with in-depth answers due to data formatting. Its reduced computational requirements make it a viable option for real-time applications.
            \item \textbf{Llama:} Demonstrated the highest performance among all models, with the highest BLEU and ROUGE scores and the lowest perplexity. The use of the Unsloth framework for fine-tuning further enhanced its performance, making it the most effective model in this study.
            \item \textbf{RoBERTa:} Showed strong performance in understanding and responding to travel-related queries, with good BLEU and ROUGE scores and lower perplexity compared to BERT and DistilBERT. However, it faced challenges in answering questions in depth due to data formatting.
        \end{itemize}

    \subsection{Future Work}

        Future work will focus on further optimizing these models to improve their performance and efficiency. Specific areas for future research include:
        \begin{itemize}
            \item Exploring additional NLP models and architectures to identify further improvements.
            \item Enhancing the data collection and preprocessing methods to increase the quality and diversity of the training data.
            \item Integrating user feedback mechanisms to continuously refine and personalize the travel itinerary recommendations.
            \item Expanding the application of these models to other domains within the travel industry, such as real-time travel assistance and dynamic itinerary adjustments based on user preferences and external factors.
        \end{itemize}

    \subsection{Potential Applications}

        The findings of this study highlight the potential of advanced NLP techniques in revolutionizing the travel planning experience. The successful implementation of these models in Roamify demonstrates their capability to provide highly accurate and personalized travel itineraries. Potential applications of this technology include:
        \begin{itemize}
            \item Personalized travel planning services that cater to individual preferences and constraints.
            \item Real-time travel assistance tools that offer dynamic recommendations based on changing conditions and user inputs.
            \item Enhanced customer support systems that leverage NLP models to provide quick and accurate responses to travel-related inquiries.
        \end{itemize}

        In conclusion, the integration of advanced NLP models into travel planning tools like Roamify represents a significant step forward in enhancing the efficiency and personalization of travel experiences. Continued research and development in this area will further unlock the potential of AI in the travel industry, providing travelers with more intuitive, responsive, and enjoyable planning experiences.

        For more information and to view our work, checkout our GitHub repository: \href{https://github.com/RoamifyRedefined/Machine-Learning}{\textbf{RoamifyRedefined/Machine-Learning}}.

\section{User Survey and Interview Analysis}

    \subsection{Survey Methodology}
        To understand the travel planning behaviors and preferences of potential users for Roamify, we conducted a comprehensive survey targeting various demographic groups. The survey consisted of multiple-choice questions and open-ended responses designed to capture detailed insights into users' travel habits, challenges faced in travel planning, and their expectations from a travel planning tool. The survey and interview questions and responses can be accessed through the following Google Drive link: \href{https://drive.google.com/drive/folders/1mKPTXZ7n7ZFmMEK6QBOKU0op3mf8hCta?usp=sharing}{\textbf{Survey Interviews}}.

    \subsection{Survey Results and Analysis}
        The survey received responses from 71 participants, covering a diverse range of age groups and travel frequencies. The key findings from the survey are summarized below:

        \begin{itemize}
            \item \textbf{Demographic Breakdown}: The majority of respondents were young adults (19-25 years) and adults (26-64 years), indicating that travel planning is a significant activity among working professionals and young individuals.
            \item \textbf{Travel Frequency}: Most respondents travel once every 6 months or once a year, highlighting a preference for occasional travel due to work and life commitments.
            \item \textbf{Types of Travel}: Casual, entertainment, and work travel were the most common, underscoring the importance of leisure and professional commitments in travel plans.
            \item \textbf{Itinerary Planning}: Online travel websites and YouTube videos are the primary sources for itinerary planning, showing a shift towards digital platforms and social media for travel information.
            \item \textbf{Customization Preferences}: A significant majority prefer customized itineraries over preselected packages, emphasizing the need for personalized travel planning tools.
            \item \textbf{Satisfaction with Travel Agencies}: Satisfaction levels with travel agencies vary, with many users expressing moderate satisfaction, indicating potential areas for improvement in travel agency services.
            \item \textbf{Challenges in Planning}: The main challenges include the overwhelming amount of information, coordination difficulties, and lack of personalized recommendations, making travel planning a time-consuming and stressful process.
        \end{itemize}

    \subsection{Interview Methodology and Findings}
        In addition to the survey, we conducted in-depth interviews with selected participants to gain deeper insights into their travel planning experiences. The interviews focused on understanding the specific pain points and expectations from a travel planning tool.

        \textbf{Key Insights from Interviews:}

        \begin{itemize}
            \item \textbf{Frequency of Travel}: Interviewees typically travel 3-4 times a year, with a mix of long vacations and short trips, emphasizing the need for flexible and comprehensive planning tools.
            \item \textbf{Planning Process}: The typical process involves extensive research using multiple online sources, comparison of prices, and seeking recommendations from friends and family, highlighting the complexity and time-consuming nature of travel planning.
            \item \textbf{Challenges}: Common challenges include dealing with an overwhelming amount of information, coordinating different trip elements, and finding reliable, personalized recommendations. These issues often lead to stress and inefficiencies in planning.
            \item \textbf{Expectations from AI Tools}: Participants expressed a strong preference for AI tools that provide tailored recommendations, streamline the booking process, and offer real-time updates. The ability to integrate multiple information sources and provide a cohesive planning experience was deemed essential.
            \item \textbf{Personalization}: The importance of personalization was reiterated, with users seeking tools that can adapt to their unique interests and preferences, ensuring a more enjoyable and relevant travel experience.
            \item \textbf{User Experience}: There is a high demand for user-friendly interfaces that offer seamless integration with other platforms, clear information presentation, and robust customer support.
        \end{itemize}

    \subsection{Ongoing Implications}
        The survey and interview findings underscore the need for a comprehensive, personalized travel planning tool like Roamify. Users face significant challenges in planning their trips, including information overload, coordination difficulties, and lack of personalized recommendations. By leveraging AI to provide tailored itineraries, real-time updates, and an intuitive user interface, Roamify can address these pain points and enhance the overall travel planning experience. The insights gained from this research will inform the development of Roamify, ensuring it meets the needs and expectations of modern travelers.

\section{Conclusion}
    The conclusion goes here.

\section*{Acknowledgment}
    The authors would like to extend their sincerest gratitude to \href{https://www.iiitd.ac.in/dhruv}{Dr Dhruv Kumar} \textit{(Computer Science \& Engineering Dept., \href{https://www.iiitd.ac.in/}{IIIT-Delhi})} for their invaluable guidance throughout the project. Their insightful feedback and expertise have been instrumental in shaping this project into its final form.


\begin{thebibliography}{1}

\bibitem{IEEEhowto:kopka}
    @article{article,
    author = {Filho, Angelo and Morabito, Reinaldo},
    year = {2023},
    month = {11},
    pages = {122437},
    title = {An effective approach for bi-objective multi-period touristic itinerary planning},
    volume = {240},
    journal = {Expert Systems with Applications},
    doi = {10.1016/j.eswa.2023.122437}
    }

\bibitem{IEEEhowto:openai}
    OpenAI. (2023). "ChatGPT Release Notes." Retrieved from \url{https://help.openai.com/en/articles/6825453-chatgpt-release-notes}.

\bibitem{IEEEhowto:econsultancy}
    Econsultancy. (2023). "Travel: How OTAs are adding generative AI to test the future of trip planning." Retrieved from \url{https://econsultancy.com/travel-ota-generative-ai/}.

\bibitem{IEEEhowto:srdvtechnologies}
    SRDV Technologies. (2023). "Major Challenges Faced by Online Travel Agencies." Retrieved from \url{https://www.srdvtechnologies.com/blog/major-challenges-faced-by-online-travel-agencies}.

\bibitem{ip2location}
    IP2Location, \emph{IP2Location IATA ICAO CSV}, \href{https://github.com/ip2location/ip2location-iata-icao/blob/master/iata-icao.csv}{https://github.com/ip2location/ip2location-iata-icao/blob/master/iata-icao.csv}.

\bibitem{transformers1}
    Hugging Face, \emph{Transformers Issue 27985 (with tracking parameters)}, \href{https://github.com/huggingface/transformers/issues/27985&sa=U&ved=2ahUKEwi175agp4GHAxXzSGwGHWIaDTEQFnoECBEQAQ&usg=AOvVaw21u1cvH6lRqHqu2CeLBNGD}{https://github.com/huggingface/transformers/issues/27985&sa=U&ved=2ahUKEwi175agp4GHAxXzSGwGHWIaDTEQFnoECBEQAQ&usg=AOvVaw21u1cvH6lRqHqu2CeLBNGD}.

\bibitem{transformers2}
    Hugging Face, \emph{Transformers Issue 27985}, \href{https://github.com/huggingface/transformers/issues/27985}{https://github.com/huggingface/transformers/issues/27985}.

\end{thebibliography}

\newpage

\appendix

\section{User Survey}
    To understand the travel planning behaviors and preferences of potential users for Roamify, we conducted a comprehensive survey. Below are the survey questions and the options provided for multiple-choice questions.

    \begin{enumerate}
        \item \textbf{What is your name?}
        
        \item \textbf{Which age group do you belong to?}
        \begin{itemize}
            \item Middle Childhood (6-11 years)
            \item Adolescents (12-18 years)
            \item Young Adults (19-25 years)
            \item Adults (26-64 years)
            \item Seniors (65 years+)
        \end{itemize}
        
        \item \textbf{How frequently do you travel?}
        \begin{itemize}
            \item Once a month
            \item Once every 3 months
            \item Once every 6 months
            \item Once a year
            \item Other
        \end{itemize}
        
        \item \textbf{What is the type of travel you usually undergo?}
        \begin{itemize}
            \item Work
            \item Entertainment
            \item Casual
            \item Meetup
            \item Other
        \end{itemize}
        
        \item \textbf{How do you decide your itinerary?}
        \begin{itemize}
            \item Travel Agents
            \item YouTube videos
            \item Online Travel websites
            \item Other
        \end{itemize}
        
        \item \textbf{Do you like to travel based on your own customized itinerary or preselected recommended tourist packages?}
        \begin{itemize}
            \item Own Customized
            \item Preselected Recommended Tourist Packages
            \item Other
        \end{itemize}
        
        \item \textbf{If you have used a travel agency how much are you satisfied with their itinerary?}
        \begin{itemize}
            \item 1
            \item 2
            \item 3
            \item 4
            \item 5
        \end{itemize}
        
        \item \textbf{If you make custom itinerary how long do you take to build one?}
        \begin{itemize}
            \item Open-ended response
        \end{itemize}
        
        \item \textbf{Do you ask your friends and family for help who are currently living or have visited that place?}
        \begin{itemize}
            \item Yes
            \item No
        \end{itemize}
    \end{enumerate}

\section{Interview Questions}
    In addition to the survey, we conducted in-depth interviews with selected participants to gain deeper insights into their travel planning experiences. Below are the interview questions:

    \begin{enumerate}
        \item How frequently do you travel?
        \item Can you describe your typical process for planning a trip?
        \item What challenges do you face when planning a trip online?
        \item Have you ever used an offline travel agency? How was your experience compared to planning trips online?
        \item What features would you find most helpful in a travel planning tool?
        \item How important is the personalization of travel recommendations to you?
        \item Can you provide an example of a travel planning experience that was particularly stressful or frustrating?
        \item How do you think AI can improve the travel planning experience?
        \item What do you expect from a travel planning app or tool in terms of user experience?
        \item Have you ever used an AI-driven travel planning website where you can ask for personalized travel suggestions? If so, how was your experience?
    \end{enumerate}

\end{document}